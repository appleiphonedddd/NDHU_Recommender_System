{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agzu6tKpRy3j",
        "outputId": "df5bf8f1-372e-4ff4-d62e-1857b9764585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3163738 sha256=ba301c247bf8ca45058e40ca241fc7f13438d3ee306307102f1fb8f51ec0c112\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSE of Content-Based**\n",
        "\n",
        "Use Cosine Similarity"
      ],
      "metadata": {
        "id": "kAHwddInp5c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from surprise import AlgoBase, Dataset, PredictionImpossible\n",
        "from surprise.model_selection import cross_validate\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "\n",
        "# Load the built-in MovieLens 100k data\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Create a dummy sparse cosine similarity matrix\n",
        "num_items = 1682  # Update this to the actual number of items in your dataset\n",
        "density = 0.01  # The density of the matrix\n",
        "data_points = np.random.choice([0, 1], size=(num_items, num_items), p=[1 - density, density])\n",
        "cosine_sim_sparse = csr_matrix(data_points)\n",
        "cosine_sim_sparse.setdiag(np.ones(num_items))\n",
        "\n",
        "# Define a custom algorithm using the cosine similarity\n",
        "class CosineSimilarityBasedAlgorithm(AlgoBase):\n",
        "    def __init__(self, cosine_sim, k=10):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.cosine_sim = cosine_sim\n",
        "        self.k = k  # Number of top similar items to consider\n",
        "        self.similarity_cache = {}  # Cache for storing item similarities\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "        return self\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unknown.')\n",
        "\n",
        "        if i not in self.similarity_cache:\n",
        "            # Compute top-k similar items\n",
        "            similarities = self.cosine_sim[i].toarray().ravel()\n",
        "            top_k_items = np.argsort(similarities)[-self.k:][::-1]\n",
        "            self.similarity_cache[i] = top_k_items\n",
        "\n",
        "        neighbors = [(alt_iid, self.cosine_sim[i, alt_iid])\n",
        "                     for alt_iid in self.similarity_cache[i]\n",
        "                     if self.trainset.knows_item(alt_iid) and alt_iid != i]\n",
        "\n",
        "        if not neighbors:\n",
        "            raise PredictionImpossible('No neighbors')\n",
        "\n",
        "        sim_total = weighted_sum = 0\n",
        "        for alt_iid, similarity in neighbors:\n",
        "          for (item, rating) in self.trainset.ur[u]:\n",
        "            if item == alt_iid:\n",
        "              similarity_value = similarity  # similarity is already an integer\n",
        "              sim_total += similarity_value\n",
        "              weighted_sum += similarity_value * rating\n",
        "              break\n",
        "          if sim_total == 0:\n",
        "            raise PredictionImpossible('No neighbors with non-zero similarity')\n",
        "\n",
        "        predicted_rating = weighted_sum / sim_total if sim_total else 0\n",
        "        return predicted_rating\n",
        "\n",
        "# Instantiate the custom algorithm with the precomputed cosine similarity matrix\n",
        "algo = CosineSimilarityBasedAlgorithm(cosine_sim=cosine_sim_sparse, k=10)\n",
        "\n",
        "# Evaluate the performance on the dataset\n",
        "results = cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GJ3GzUllmDf",
        "outputId": "27741657-5398-4dc3-d893-26a2dd7afd0a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RMSE of algorithm CosineSimilarityBasedAlgorithm on 3 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
            "RMSE (testset)    1.1440  1.1395  1.1437  1.1424  0.0020  \n",
            "Fit time          0.00    0.02    0.02    0.01    0.01    \n",
            "Test time         9.31    10.96   13.09   11.12   1.55    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NDCG of Content-Based**"
      ],
      "metadata": {
        "id": "clzZf1g-qDSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom algorithm with the precomputed cosine similarity matrix\n",
        "algo = CosineSimilarityBasedAlgorithm(cosine_sim=cosine_sim_sparse, k=10)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "from surprise.model_selection import train_test_split\n",
        "trainset, testset = train_test_split(data, test_size=0.25)\n",
        "\n",
        "# Train the algorithm on the training set\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make recommendations for all users in the test set\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Organize predictions into a dictionary where keys are user IDs and values are lists of recommended item IDs\n",
        "user_predictions = {}\n",
        "for prediction in predictions:\n",
        "    user_id = prediction.uid\n",
        "    item_id = prediction.iid\n",
        "    estimated_rating = prediction.est\n",
        "    if user_id not in user_predictions:\n",
        "        user_predictions[user_id] = []\n",
        "    user_predictions[user_id].append((item_id, estimated_rating))\n",
        "\n",
        "# Calculate NDCG for each user\n",
        "\n",
        "user_true_ratings = {}\n",
        "for uid, iid, true_r in testset:\n",
        "    if uid not in user_true_ratings:\n",
        "        user_true_ratings[uid] = {}\n",
        "    user_true_ratings[uid][iid] = true_r\n",
        "\n",
        "# calcuate every client NDCG\n",
        "ndcg_scores = []\n",
        "for uid, user_ratings in user_predictions.items():\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    true_ratings = [user_true_ratings[uid][iid] if iid in user_true_ratings[uid] else 0 for iid, _ in user_ratings[:10]]\n",
        "    estimated_ratings = [rating for _, rating in user_ratings[:10]]\n",
        "    dcg = sum([(2 ** true - 1) / np.log2(i + 2) for i, true in enumerate(true_ratings)])\n",
        "    idcg = sum([(2 ** true - 1) / np.log2(i + 2) for i, true in enumerate(sorted(true_ratings, reverse=True))])\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0\n",
        "    ndcg_scores.append(ndcg)\n",
        "\n",
        "average_ndcg = np.mean(ndcg_scores)\n",
        "print(\"Average NDCG:\", average_ndcg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbyOWIleoEod",
        "outputId": "e7152a07-d295-4822-a61f-954695fe7af1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average NDCG: 0.823449228981161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSE of SVD**"
      ],
      "metadata": {
        "id": "Asa-RfAOrj_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to the dataset file\n",
        "file_path = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# As in your previous code, define the reader with the correct format\n",
        "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
        "\n",
        "# Define the SVD algorithm\n",
        "algo = SVD()\n",
        "\n",
        "# Run 5-fold cross-validation and print results\n",
        "cross_validate(algo, file_path, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU9hF8_kTtuI",
        "outputId": "dece3511-7349-498c-9e42-99900d860851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    0.9399  0.9436  0.9301  0.9310  0.9332  0.9356  0.0053  \n",
            "MAE (testset)     0.7426  0.7429  0.7347  0.7332  0.7349  0.7376  0.0042  \n",
            "Fit time          1.43    1.43    1.42    1.46    2.15    1.58    0.29    \n",
            "Test time         0.20    0.13    0.44    0.12    0.22    0.22    0.12    \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_rmse': array([0.93985157, 0.94361622, 0.93008772, 0.93099408, 0.9332488 ]),\n",
              " 'test_mae': array([0.74261409, 0.74285568, 0.73465993, 0.73315292, 0.73486236]),\n",
              " 'fit_time': (1.430532455444336,\n",
              "  1.4265687465667725,\n",
              "  1.4150314331054688,\n",
              "  1.4574902057647705,\n",
              "  2.15321683883667),\n",
              " 'test_time': (0.20369362831115723,\n",
              "  0.1315441131591797,\n",
              "  0.4414336681365967,\n",
              "  0.12229442596435547,\n",
              "  0.22421526908874512)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NDCG of SVD**"
      ],
      "metadata": {
        "id": "UPqGJ50HrsrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Define and train the SVD model\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Custom function to calculate NDCG\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    # Group the prediction scores by user\n",
        "    user_est_true = {}\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        if uid not in user_est_true:\n",
        "            user_est_true[uid] = []\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    ndcg = 0\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Keep only the top k items\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings = user_ratings[:k]\n",
        "\n",
        "        # Calculate DCG (Discounted Cumulative Gain) and IDCG (Ideal DCG)\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings)])\n",
        "        idcg = sum([np.log2(i + 2) for i in range(len(user_ratings))])\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    # Calculate the average NDCG\n",
        "    return ndcg / len(user_est_true)\n",
        "\n",
        "# Calculate NDCG\n",
        "ndcg_value = calculate_ndcg(predictions, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpDSBDBqacG3",
        "outputId": "fd07511a-9d8f-460a-c3e5-5a1c9425000f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG: 0.9209504395469672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSE of GridSearchCV**"
      ],
      "metadata": {
        "id": "aL8zXQQgv9Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# Load the movielens-100k dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Define a parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_epochs': [5, 10], # Number of epochs. You can try different numbers here.\n",
        "    'lr_all': [0.002, 0.005], # Learning rate. You can try different values here.\n",
        "    'reg_all': [0.4, 0.6] # Regularization term. You can try different values here.\n",
        "}\n",
        "\n",
        "# Setup the grid search\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "\n",
        "# Perform the grid search\n",
        "gs.fit(data)\n",
        "\n",
        "# Best RMSE score\n",
        "print(gs.best_score['rmse'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ReokxqZT4Ui",
        "outputId": "119b7db0-8e7f-4931-9f56-fb87b7aee5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9645225862412641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NDCG of GridSearchCV**"
      ],
      "metadata": {
        "id": "uN_OhFX9r790"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset and train the model using the best parameters found by GridSearchCV\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "best_params = gs.best_params['rmse']\n",
        "algo = SVD(n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make predictions on the testset\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Calculate_ndcg\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    user_est_true = {}\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        if uid not in user_est_true:\n",
        "            user_est_true[uid] = []\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    ndcg = 0\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings = user_ratings[:k]\n",
        "\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings)])\n",
        "        idcg = sum([np.log2(i + 2) for i in range(len(user_ratings))])\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    return ndcg / len(user_est_true)\n",
        "\n",
        "ndcg_value = calculate_ndcg(predictions, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gua1EW-jsR3",
        "outputId": "84bb1178-3856-42da-c89a-85b7c9fcc746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG: 0.9182108319784339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSE of Stacking**\n",
        "\n",
        "Use Stacking Ensemble SVD and KNN"
      ],
      "metadata": {
        "id": "z_d5bMqSew0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
        "from surprise.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Load the MovieLens 100k dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Setting up five-fold cross-validation\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "# Initialize SVD and KNN Basic algorithms\n",
        "algo_svd = SVD()\n",
        "algo_knn = KNNBasic()\n",
        "\n",
        "# Lists to store predictions from SVD and KNN\n",
        "svd_predictions = []\n",
        "knn_predictions = []\n",
        "actual_ratings = []  # List to store actual ratings\n",
        "\n",
        "# For each train/test split in the dataset\n",
        "for trainset, testset in kf.split(data):\n",
        "\n",
        "    # Train SVD model and make predictions\n",
        "    algo_svd.fit(trainset)\n",
        "    predictions_svd = algo_svd.test(testset)\n",
        "    svd_predictions.extend(predictions_svd)\n",
        "\n",
        "    # Train KNN model and make predictions\n",
        "    algo_knn.fit(trainset)\n",
        "    predictions_knn = algo_knn.test(testset)\n",
        "    knn_predictions.extend(predictions_knn)\n",
        "\n",
        "    # Extract actual ratings from the testset\n",
        "    actual_ratings.extend([rating for (_, _, rating) in testset])\n",
        "\n",
        "# Combine predictions from SVD and KNN\n",
        "combined_predictions = []\n",
        "for svd_pred, knn_pred in zip(svd_predictions, knn_predictions):\n",
        "    combined_pred = (svd_pred.est + knn_pred.est) / 2\n",
        "    combined_predictions.append(combined_pred)\n",
        "\n",
        "# Convert the predictions and actual ratings to numpy arrays for calculation\n",
        "combined_predictions_array = np.array(combined_predictions)\n",
        "actual_ratings_array = np.array(actual_ratings)\n",
        "\n",
        "# Manually calculate RMSE\n",
        "mse = np.mean((actual_ratings_array - combined_predictions_array) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f'Combined RMSE: {rmse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vWSDToDdZ_m",
        "outputId": "b8eb160c-4a21-4b80-a416-d7812aa5fe5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Combined RMSE: 0.9350701259965264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NDCG of Stacking**"
      ],
      "metadata": {
        "id": "mxzH1ZgosV8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    # Create a dictionary to store predicted ratings and true ratings grouped by user ID\n",
        "    user_est_true = defaultdict(list)\n",
        "\n",
        "    # Iterate through all prediction results and group them by user ID\n",
        "    for uid, _, true_r, est in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    # Initialize the sum of NDCG\n",
        "    ndcg = 0\n",
        "    # Iterate through predicted ratings and true ratings for each user\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort by predicted ratings in descending order\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings_k = user_ratings[:k]\n",
        "\n",
        "        # Calculate DCG (Discounted Cumulative Gain)\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings_k)])\n",
        "\n",
        "        # Sort by true ratings in descending order to calculate IDCG (Ideal Discounted Cumulative Gain)\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        ideal_ratings_k = user_ratings[:k]\n",
        "        idcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(ideal_ratings_k)])\n",
        "\n",
        "        # Calculate NDCG (Normalized Discounted Cumulative Gain) and add it to the sum\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    # Calculate the average NDCG for all users, return 0 if there are no users\n",
        "    return ndcg / len(user_est_true) if user_est_true else 0\n",
        "\n",
        "# Calculate NDCG for the combined predictions\n",
        "ndcg_value = calculate_ndcg(combined_predictions_with_details, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLuJfSyo54Y",
        "outputId": "2523bc5c-90dd-4df6-b5ad-46363bfe82b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG: 0.8638025105186771\n"
          ]
        }
      ]
    }
  ]
}