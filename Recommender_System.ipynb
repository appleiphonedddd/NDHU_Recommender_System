{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agzu6tKpRy3j",
        "outputId": "8d8649a7-cf10-4d1e-8c75-86ee93af2a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/772.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/772.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m768.0/772.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3163763 sha256=991e76acd233dbdbb3460780aa55c01612389b8adaafb7c022d87ed980f73b24\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2dGYqWJ8R4Oy"
      },
      "outputs": [],
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU9hF8_kTtuI",
        "outputId": "dece3511-7349-498c-9e42-99900d860851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "RMSE (testset)    0.9399  0.9436  0.9301  0.9310  0.9332  0.9356  0.0053  \n",
            "MAE (testset)     0.7426  0.7429  0.7347  0.7332  0.7349  0.7376  0.0042  \n",
            "Fit time          1.43    1.43    1.42    1.46    2.15    1.58    0.29    \n",
            "Test time         0.20    0.13    0.44    0.12    0.22    0.22    0.12    \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'test_rmse': array([0.93985157, 0.94361622, 0.93008772, 0.93099408, 0.9332488 ]),\n",
              " 'test_mae': array([0.74261409, 0.74285568, 0.73465993, 0.73315292, 0.73486236]),\n",
              " 'fit_time': (1.430532455444336,\n",
              "  1.4265687465667725,\n",
              "  1.4150314331054688,\n",
              "  1.4574902057647705,\n",
              "  2.15321683883667),\n",
              " 'test_time': (0.20369362831115723,\n",
              "  0.1315441131591797,\n",
              "  0.4414336681365967,\n",
              "  0.12229442596435547,\n",
              "  0.22421526908874512)}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The path to the dataset file\n",
        "file_path = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# As in your previous code, define the reader with the correct format\n",
        "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
        "\n",
        "# Define the SVD algorithm\n",
        "algo = SVD()\n",
        "\n",
        "# Run 5-fold cross-validation and print results\n",
        "cross_validate(algo, file_path, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpDSBDBqacG3",
        "outputId": "fd07511a-9d8f-460a-c3e5-5a1c9425000f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG: 0.9209504395469672\n"
          ]
        }
      ],
      "source": [
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Define and train the SVD model\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Custom function to calculate NDCG\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    # Group the prediction scores by user\n",
        "    user_est_true = {}\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        if uid not in user_est_true:\n",
        "            user_est_true[uid] = []\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    ndcg = 0\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Keep only the top k items\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings = user_ratings[:k]\n",
        "\n",
        "        # Calculate DCG (Discounted Cumulative Gain) and IDCG (Ideal DCG)\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings)])\n",
        "        idcg = sum([np.log2(i + 2) for i in range(len(user_ratings))])\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    # Calculate the average NDCG\n",
        "    return ndcg / len(user_est_true)\n",
        "\n",
        "# Calculate NDCG\n",
        "ndcg_value = calculate_ndcg(predictions, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ReokxqZT4Ui",
        "outputId": "119b7db0-8e7f-4931-9f56-fb87b7aee5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9645225862412641\n"
          ]
        }
      ],
      "source": [
        "from surprise import SVD\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# Load the movielens-100k dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Define a parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_epochs': [5, 10], # Number of epochs. You can try different numbers here.\n",
        "    'lr_all': [0.002, 0.005], # Learning rate. You can try different values here.\n",
        "    'reg_all': [0.4, 0.6] # Regularization term. You can try different values here.\n",
        "}\n",
        "\n",
        "# Setup the grid search\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "\n",
        "# Perform the grid search\n",
        "gs.fit(data)\n",
        "\n",
        "# Best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# Combination of parameters that gave the best RMSE score\n",
        "#print(gs.best_params['rmse'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gua1EW-jsR3",
        "outputId": "84bb1178-3856-42da-c89a-85b7c9fcc746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG: 0.9182108319784339\n"
          ]
        }
      ],
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset and train the model using the best parameters found by GridSearchCV\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "best_params = gs.best_params['rmse']\n",
        "algo = SVD(n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Make predictions on the testset\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# calculate_ndcg\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    user_est_true = {}\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        if uid not in user_est_true:\n",
        "            user_est_true[uid] = []\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    ndcg = 0\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings = user_ratings[:k]\n",
        "\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings)])\n",
        "        idcg = sum([np.log2(i + 2) for i in range(len(user_ratings))])\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    return ndcg / len(user_est_true)\n",
        "\n",
        "ndcg_value = calculate_ndcg(predictions, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_d5bMqSew0f"
      },
      "source": [
        "**Stacking Ensemble SVD and KNN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vWSDToDdZ_m",
        "outputId": "b8eb160c-4a21-4b80-a416-d7812aa5fe5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Combined RMSE: 0.9350701259965264\n"
          ]
        }
      ],
      "source": [
        "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
        "from surprise.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Load the MovieLens 100k dataset\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Setting up five-fold cross-validation\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "# Initialize SVD and KNN Basic algorithms\n",
        "algo_svd = SVD()\n",
        "algo_knn = KNNBasic()\n",
        "\n",
        "# Lists to store predictions from SVD and KNN\n",
        "svd_predictions = []\n",
        "knn_predictions = []\n",
        "actual_ratings = []  # List to store actual ratings\n",
        "\n",
        "# For each train/test split in the dataset\n",
        "for trainset, testset in kf.split(data):\n",
        "\n",
        "    # Train SVD model and make predictions\n",
        "    algo_svd.fit(trainset)\n",
        "    predictions_svd = algo_svd.test(testset)\n",
        "    svd_predictions.extend(predictions_svd)\n",
        "\n",
        "    # Train KNN model and make predictions\n",
        "    algo_knn.fit(trainset)\n",
        "    predictions_knn = algo_knn.test(testset)\n",
        "    knn_predictions.extend(predictions_knn)\n",
        "\n",
        "    # Extract actual ratings from the testset\n",
        "    actual_ratings.extend([rating for (_, _, rating) in testset])\n",
        "\n",
        "# Combine predictions from SVD and KNN\n",
        "combined_predictions = []\n",
        "for svd_pred, knn_pred in zip(svd_predictions, knn_predictions):\n",
        "    combined_pred = (svd_pred.est + knn_pred.est) / 2\n",
        "    combined_predictions.append(combined_pred)\n",
        "\n",
        "# Convert the predictions and actual ratings to numpy arrays for calculation\n",
        "combined_predictions_array = np.array(combined_predictions)\n",
        "actual_ratings_array = np.array(actual_ratings)\n",
        "\n",
        "# Manually calculate RMSE\n",
        "mse = np.mean((actual_ratings_array - combined_predictions_array) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f'Combined RMSE: {rmse}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLuJfSyo54Y",
        "outputId": "2523bc5c-90dd-4df6-b5ad-46363bfe82b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG: 0.8638025105186771\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def calculate_ndcg(predictions, k=10):\n",
        "    # Create a dictionary to store predicted ratings and true ratings grouped by user ID\n",
        "    user_est_true = defaultdict(list)\n",
        "\n",
        "    # Iterate through all prediction results and group them by user ID\n",
        "    for uid, _, true_r, est in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    # Initialize the sum of NDCG\n",
        "    ndcg = 0\n",
        "    # Iterate through predicted ratings and true ratings for each user\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort by predicted ratings in descending order\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        user_ratings_k = user_ratings[:k]\n",
        "\n",
        "        # Calculate DCG (Discounted Cumulative Gain)\n",
        "        dcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(user_ratings_k)])\n",
        "\n",
        "        # Sort by true ratings in descending order to calculate IDCG (Ideal Discounted Cumulative Gain)\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        ideal_ratings_k = user_ratings[:k]\n",
        "        idcg = sum([true_r / np.log2(i + 2) for i, (_, true_r) in enumerate(ideal_ratings_k)])\n",
        "\n",
        "        # Calculate NDCG (Normalized Discounted Cumulative Gain) and add it to the sum\n",
        "        ndcg += dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "    # Calculate the average NDCG for all users, return 0 if there are no users\n",
        "    return ndcg / len(user_est_true) if user_est_true else 0\n",
        "\n",
        "# Calculate NDCG for the combined predictions\n",
        "ndcg_value = calculate_ndcg(combined_predictions_with_details, k=10)\n",
        "print(f'NDCG: {ndcg_value}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
